{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d10da6-4073-40c8-8098-ccc759417552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tic/miniconda3/envs/cvpr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "# from abc import ABC\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "# import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "\n",
    "from scipy.ndimage import label\n",
    "import numpy as np\n",
    "\n",
    "import lightning as L\n",
    "from lightning.fabric.loggers import TensorBoardLogger\n",
    "from lightning.fabric.fabric import _FabricOptimizer\n",
    "\n",
    "import torchvision\n",
    "from box import Box\n",
    "from datasets import call_load_dataset\n",
    "from utils.model import Model\n",
    "from utils.losses import DiceLoss, FocalLoss, Matching_Loss\n",
    "from utils.eval_utils import AverageMeter, validate, get_prompts, calc_iou, validate_sam2\n",
    "from utils.tools import copy_model, create_csv, reduce_instances\n",
    "from utils.utils import *\n",
    "\n",
    "import  csv, copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d122bc-4b7f-4109-91d1-dc08774e6658",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "# %% [markdown]\n",
    "# ## Optimizer and Scheduler Configuration\n",
    "\n",
    "# %% [code]\n",
    "def configure_opt(cfg: Box, model: Model):\n",
    "    def lr_lambda(step):\n",
    "        if step < cfg.opt.warmup_steps:\n",
    "            return step / cfg.opt.warmup_steps\n",
    "        elif step < cfg.opt.steps[0]:\n",
    "            return 1.0\n",
    "        elif step < cfg.opt.steps[1]:\n",
    "            return 1 / cfg.opt.decay_factor\n",
    "        else:\n",
    "            return 1 / (cfg.opt.decay_factor**2)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.model.parameters(),\n",
    "                                 lr=cfg.opt.learning_rate,\n",
    "                                 weight_decay=cfg.opt.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b57c27-ed2b-449b-95cc-61d068d98975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_forward(img_tensor, prompt, model):\n",
    "    with torch.no_grad():\n",
    "        _, logits, _, _ = model(img_tensor, prompt)\n",
    "\n",
    "    \n",
    "    entropy_maps = []\n",
    "    masks_pred = []\n",
    "    eps = 1e-8\n",
    "    for logits_p in logits[0]:  # or just masks_pred if it's already a list\n",
    "        # Entropy per pixel\n",
    "        mask_p = torch.sigmoid(logits_p)\n",
    "        entropy = - (mask_p * torch.log(mask_p + eps) + (1 - mask_p) * torch.log(1 - mask_p + eps))\n",
    "    \n",
    "  \n",
    "        max_ent = torch.log(torch.tensor(2.0, device=mask_p.device))\n",
    "        entropy_norm = entropy / (max_ent + 1e-8)   # [0, 1]\n",
    "                \n",
    "        entropy_maps.append(entropy_norm)\n",
    "        masks_pred.append(mask_p)\n",
    "\n",
    "    return entropy_maps, masks_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194d20e1-9b1c-4e2c-8a57-a8277feb535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_map_calculate(p):\n",
    "    eps = 1e-8\n",
    "    p = p.clamp(eps, 1 - eps)  # Safe!\n",
    "    entropy_map = - (p * torch.log(p) + (1 - p) * torch.log(1 - p))\n",
    "    # entropy_map = entropy_map.max(dim=0)[0]\n",
    "    return entropy_map# / torch.log(torch.tensor(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117b8954-1e30-42ac-8e76-95a009fb352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_feature(embedding_map, bbox, stride=16, pooling='avg'):\n",
    "    \"\"\"\n",
    "    Extract a feature vector from an embedding map given a bounding box.\n",
    "    \n",
    "    Args:\n",
    "        embedding_map (torch.Tensor): Shape (C, H_feat, W_feat) or (B, C, H_feat, W_feat)\n",
    "        bbox (list or torch.Tensor): [x1, y1, x2, y2] in original image coordinates\n",
    "        stride (int): Downscaling factor between image and feature map\n",
    "        pooling (str): 'avg' or 'max' pooling inside the bbox region\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Feature vector of shape (C,)\n",
    "    \"\"\"\n",
    "    # If batch dimension exists, assume batch size 1\n",
    "    if embedding_map.dim() == 4:\n",
    "        embedding_map = embedding_map[0]\n",
    "\n",
    "    C, H_feat, W_feat = embedding_map.shape\n",
    "    x1, y1, x2, y2 = bbox\n",
    "\n",
    "    # Map bbox to feature map coordinates\n",
    "    fx1 = max(int(x1 / stride), 0)\n",
    "    fy1 = max(int(y1 / stride), 0)\n",
    "    fx2 = min(int((x2 + stride - 1) / stride), W_feat)  # ceil division\n",
    "    fy2 = min(int((y2 + stride - 1) / stride), H_feat)\n",
    "\n",
    "    # Crop the feature map to bbox region\n",
    "    region = embedding_map[:, fy1:fy2, fx1:fx2]\n",
    "\n",
    "    if region.numel() == 0:\n",
    "        # fallback to global feature if bbox is too small\n",
    "        region = embedding_map\n",
    "\n",
    "    # Pool to get a single feature vector\n",
    "    if pooling == 'avg':\n",
    "        feature_vec = region.mean(dim=(1,2))\n",
    "    elif pooling == 'max':\n",
    "        feature_vec = region.amax(dim=(1,2))\n",
    "    else:\n",
    "        raise ValueError(\"pooling must be 'avg' or 'max'\")\n",
    "\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "121b5181-f324-45ab-88af-dc296f6eca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_opt(cfg: Box, model: Model):\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if step < cfg.opt.warmup_steps:\n",
    "            return step / cfg.opt.warmup_steps\n",
    "        elif step < cfg.opt.steps[0]:\n",
    "            return 1.0\n",
    "        elif step < cfg.opt.steps[1]:\n",
    "            return 1 / cfg.opt.decay_factor\n",
    "        else:\n",
    "            return 1 / (cfg.opt.decay_factor**2)\n",
    "\n",
    "    # optimize only trainable params (e.g., LoRA)\n",
    "    trainable_params = (p for p in model.model.parameters() if p.requires_grad)\n",
    "    optimizer = torch.optim.Adam(trainable_params, lr=cfg.opt.learning_rate, weight_decay=cfg.opt.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a369bcfd-3e11-4b0f-85ca-00e9399d73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f08d8e08-461c-4082-9d3e-bda1442d7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def visualize_instance_segmentation(images_weak, gt_masks_new,prompts, entropy_maps, preds,\n",
    "                                    invert_overlap_map, soft_masks, bboxes, j=0, slice_step=50):\n",
    "    \"\"\"\n",
    "    Visualizes:\n",
    "    1. The weak input image\n",
    "    2. Ground-truth instance masks\n",
    "    3. Entropy heatmaps\n",
    "    4. Predicted masks\n",
    "    5. Bounding boxes overlaid on image\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Convert image tensor to numpy\n",
    "    img = images_weak[0].detach().cpu()\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    if img.min() < 0:\n",
    "        img = img * std + mean\n",
    "    img_np = TF.to_pil_image(img.clamp(0, 1))\n",
    "    img_np = np.array(img_np)\n",
    "\n",
    "    # --- Shapes\n",
    "    num_instances = gt_masks_new.shape[1]\n",
    "    H, W = gt_masks_new.shape[-2:]\n",
    "    print(f\"ðŸ”¹ Visualizing {num_instances} instances, each {H}Ã—{W}\")\n",
    "\n",
    "    # --- Convert tensors to numpy\n",
    "    gt_masks_np = gt_masks_new[0].detach().cpu().numpy()\n",
    "    entropy_maps_np = entropy_maps[0].detach().cpu().numpy()\n",
    "    preds_np = preds.detach().cpu().numpy()[0]\n",
    "    print(preds_np.min())\n",
    "   \n",
    "\n",
    "        # --- Plot image with bounding boxes AND prompt points\n",
    "    print(\"Original Image with Bounding Boxes and Prompt Points\")\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(img_np)\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    if bboxes.max() <= 1.5:\n",
    "        bboxes_scaled = bboxes.clone()\n",
    "        bboxes_scaled[:, [0, 2]] *= W\n",
    "        bboxes_scaled[:, [1, 3]] *= H\n",
    "    else:\n",
    "        bboxes_scaled = bboxes\n",
    "\n",
    "    for idx, box in enumerate(bboxes_scaled):\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        width, height = x2 - x1, y2 - y1\n",
    "        rect = Rectangle((x1, y1), width, height,\n",
    "                         linewidth=2, edgecolor='lime', facecolor='none',\n",
    "                         label='BBox' if idx == 0 else \"\")\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # === PLOT PROMPT POINTS ===\n",
    "    try:\n",
    "        point_coords = prompts[0][0].detach().cpu().numpy()  # [N, 2]\n",
    "        point_labels = prompts[0][1].detach().cpu().numpy()  # [N,]\n",
    "\n",
    "        # Scale coordinates if normalized\n",
    "        if point_coords.max() <= 1.5:\n",
    "            point_coords_px = point_coords.copy()\n",
    "            point_coords_px[:, 0] *= W\n",
    "            point_coords_px[:, 1] *= H\n",
    "        else:\n",
    "            point_coords_px = point_coords\n",
    "\n",
    "        pos_points = point_coords_px[point_labels == 1]\n",
    "        neg_points = point_coords_px[point_labels == 0]\n",
    "\n",
    "        if len(pos_points) > 0:\n",
    "            ax.scatter(pos_points[:, 0], pos_points[:, 1], c='green', s=80, marker='o',\n",
    "                       edgecolors='white', linewidths=1.5, label='Positive Prompt', zorder=5)\n",
    "        if len(neg_points) > 0:\n",
    "            ax.scatter(neg_points[:, 0], neg_points[:, 1], c='red', s=80, marker='x',\n",
    "                       linewidths=3, label='Negative Prompt', zorder=5)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot prompts: {e}\")\n",
    "\n",
    "    ax.set_title(f\"Image + BBoxes + Prompts (Inst: {num_instances})\")\n",
    "    ax.axis('off')\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    if handles:\n",
    "        ax.legend(handles, labels, loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Visualize per-instance details\n",
    "    n_cols = 4\n",
    "    n_rows = min(num_instances, 5)\n",
    "    for i in range(n_rows):\n",
    "        gt_mask = gt_masks_np[i]\n",
    "        gt_colored = img_np.copy()\n",
    "        gt_mask_rgb = cv2.applyColorMap((gt_mask * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "        gt_mask_rgb = cv2.cvtColor(gt_mask_rgb, cv2.COLOR_BGR2RGB)\n",
    "        gt_overlay = cv2.addWeighted(gt_mask_rgb, 0.5, gt_colored, 0.5, 0)\n",
    "\n",
    "        entropy = entropy_maps_np[i]\n",
    "        entropy_norm = (entropy - entropy.min()) / (entropy.max() - entropy.min() + 1e-8)\n",
    "        entropy_heatmap = cv2.applyColorMap((entropy_norm * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "        entropy_heatmap = cv2.cvtColor(entropy_heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        pred_mask = preds_np[i]\n",
    "        pred_colored = img_np.copy()\n",
    "        pred_mask_rgb = cv2.applyColorMap((pred_mask * 255).astype(np.uint8), cv2.COLORMAP_PARULA)\n",
    "        pred_mask_rgb = cv2.cvtColor(pred_mask_rgb, cv2.COLOR_BGR2RGB)\n",
    "        pred_overlay = cv2.addWeighted(pred_mask_rgb, 0.5, pred_colored, 0.5, 0)\n",
    "\n",
    "        print(f\"Instance {i+1} GT mask\")\n",
    "        plt.imshow(gt_mask, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Instance {i+1} Entropy\")\n",
    "        plt.imshow(entropy_norm, cmap='viridis')\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Instance {i+1} Predicted mask\")\n",
    "        plt.imshow(pred_mask > 0.95, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    print(\"Inverted overlap\")\n",
    "    plt.imshow(invert_overlap_map.detach().cpu().numpy(), cmap='viridis')\n",
    "    plt.show()\n",
    "\n",
    "    pred_one = (preds_np > 0.5).sum(axis=0)\n",
    "    print(\"Pred all\")\n",
    "    plt.imshow(pred_one > 0.5, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Pseudo labels\")\n",
    "\n",
    " \n",
    "  \n",
    "    pseudo_lab = torch.sigmoid(torch.stack(soft_masks, dim=0)).sum(dim=1)\n",
    "    print(pseudo_lab.shape)\n",
    "    print(pseudo_lab.max())\n",
    "    plt.imshow((pseudo_lab[0].detach().cpu().numpy() > 0.5), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Bounding boxes tensor:\", bboxes.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b02bfe35-be35-42ce-9f64-76d59fc5b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe6694c-1823-4aba-a32a-711f3c718218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sam(\n",
    "    cfg: Box,\n",
    "    fabric: L.Fabric,\n",
    "    model: Model,\n",
    "    optimizer: _FabricOptimizer,\n",
    "    scheduler: _FabricOptimizer,\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    init_iou, \n",
    "    vis\n",
    "):\n",
    "\n",
    "    focal_loss = FocalLoss()\n",
    "    dice_loss = DiceLoss()\n",
    "    best_iou = init_iou\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    no_improve_count = 0\n",
    "    max_patience = cfg.get(\"patience\", 3)  # stop if no improvement for X validations\n",
    "    match_interval = cfg.match_interval\n",
    "    eval_interval = int(len(train_dataloader) * 1)\n",
    "\n",
    "    window_size = 100\n",
    "    embedding_queue = []\n",
    "    ite_em = 0\n",
    "\n",
    "    # Prepare output dirs\n",
    "    os.makedirs(os.path.join(cfg.out_dir, \"save\"), exist_ok=True)\n",
    "    csv_path = os.path.join(cfg.out_dir, \"training_log.csv\")\n",
    "\n",
    "    # Initialize CSV\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Epoch\", \"Iteration\", \"Val_IoU\", \"Best_IoU\", \"Status\"])\n",
    "\n",
    "    fabric.print(f\"Training with rollback enabled. Logging to: {csv_path}\")\n",
    "\n",
    "\n",
    "    for epoch in range(1, cfg.num_epochs + 1):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        focal_losses = AverageMeter()\n",
    "        dice_losses = AverageMeter()\n",
    "        iou_losses = AverageMeter()\n",
    "        total_losses = AverageMeter()\n",
    "        sim_losses = AverageMeter()\n",
    "        end = time.time()\n",
    "        # x_rnd = random.randint(0, len(train_dataloader))\n",
    "   \n",
    "        idx = random.randint(0, len(train_dataloader) - 1)\n",
    "        data = train_dataloader.dataset[10]\n",
    "        print(idx)\n",
    "      \n",
    "        for iter, data in enumerate(train_dataloader):\n",
    "    \n",
    "            data_time.update(time.time() - end)\n",
    "            images_weak, images_strong, bboxes_gt, gt_masks, img_paths= data\n",
    "            del data\n",
    "    \n",
    "            \n",
    "    \n",
    "            if iter==23:\n",
    "                slice_step = 50\n",
    "                for j in range(0, len(gt_masks[0]), slice_step):\n",
    "\n",
    "                    \n",
    "              \n",
    "                    gt_masks_new = gt_masks[0][j:j+slice_step].unsqueeze(0)\n",
    "       \n",
    "                    prompts = get_prompts(cfg, bboxes_gt, gt_masks_new)\n",
    "    \n",
    "                    batch_size = images_weak.size(0)\n",
    "    \n",
    "    \n",
    "                    entropy_maps, preds = process_forward(images_weak, prompts, model)\n",
    "                    preds = torch.stack(preds, dim=0).unsqueeze(0)\n",
    "                    entropy_maps = torch.stack(entropy_maps, dim=0).unsqueeze(0)\n",
    "    \n",
    "    \n",
    "                    pred_binary = ((preds[0]* (1-entropy_maps[0])) >0.99).float() #* (1-entropy_maps[0]) #(pred_stack>0.95 ) & \n",
    "    \n",
    "                    \n",
    "                    \n",
    "    \n",
    "                    overlap_count = pred_binary.sum(dim=0)  \n",
    "                    overlap_map = (overlap_count > 1).float()\n",
    "                    invert_overlap_map = 1.0 - overlap_map\n",
    "    \n",
    "                    \n",
    "                 \n",
    "                    bboxes = []\n",
    "                    point_list = []\n",
    "                    point_labels_list = []\n",
    "    \n",
    "                   \n",
    "                    for i,  pred in enumerate( preds[0]):\n",
    "                        point_coords = prompts[0][0][i][:].unsqueeze(0)\n",
    "                        point_coords_lab = prompts[0][1][i][:].unsqueeze(0)\n",
    "                      \n",
    "                        pred_without_overlap = (pred>0.99) * invert_overlap_map\n",
    "                  \n",
    "                        \n",
    "                        \n",
    "                        ys, xs = torch.where(pred_without_overlap> 0.5)\n",
    "                        if len(xs) > 0 and len(ys) > 0:\n",
    "                            x_min, x_max = xs.min().item(), xs.max().item()\n",
    "                            y_min, y_max = ys.min().item(), ys.max().item()\n",
    "                            bboxes.append(torch.tensor([x_min, y_min , x_max, y_max], dtype=torch.float32))\n",
    "    \n",
    "                            point_list.append(point_coords)\n",
    "                            point_labels_list.append(point_coords_lab)\n",
    "                    \n",
    "                    if len(bboxes) == 0:\n",
    "                        continue  # skip if no valid region\n",
    "                   \n",
    "                            \n",
    "                    point_ = torch.cat(point_list).squeeze(1)\n",
    "                    point_labels_ = torch.cat(point_labels_list)\n",
    "                    new_prompts = [(point_, point_labels_)]\n",
    "                \n",
    "                    bboxes = torch.stack(bboxes)\n",
    "    \n",
    "                    \n",
    "    \n",
    "                    with torch.no_grad():\n",
    "                        embeddings, soft_masks, _, _ = model(images_weak, bboxes.unsqueeze(0))\n",
    "                        \n",
    "      \n",
    "                    _, pred_masks, iou_predictions, _= model(images_strong, new_prompts)\n",
    "                    del _\n",
    "               \n",
    "    \n",
    "                    # print(preds[0].shape)\n",
    "                    # print(soft_masks[0].shape)\n",
    "                     # Visualization\n",
    "                    visualize_instance_segmentation(images_weak, gt_masks_new, prompts, entropy_maps, preds, invert_overlap_map, soft_masks, bboxes)\n",
    "    \n",
    "                    num_masks = sum(len(pred_mask) for pred_mask in pred_masks)\n",
    "                    loss_focal = torch.tensor(0., device=fabric.device)\n",
    "                    loss_dice = torch.tensor(0., device=fabric.device)\n",
    "                    loss_iou = torch.tensor(0., device=fabric.device)\n",
    "                    loss_sim = torch.tensor(0., device=fabric.device)\n",
    "    \n",
    "                    \n",
    "    \n",
    "                    for i, (pred_mask, soft_mask, iou_prediction, bbox) in enumerate(\n",
    "                            zip(pred_masks, soft_masks, iou_predictions, bboxes  )\n",
    "                        ):  \n",
    "    \n",
    "                            embed_feats = get_bbox_feature( embeddings, bbox)\n",
    "                            embed_feats = F.normalize(embed_feats, p=2, dim=0)\n",
    "                            embedding_queue.append(embed_feats)\n",
    "                            loss_match = 0\n",
    "                            \n",
    "                           \n",
    "                            if len(embedding_queue) > 1:\n",
    "                                # Stack all embeddings (num_instances, feature_dim)\n",
    "                                features = torch.stack(embedding_queue, dim=0)  # [N, D]\n",
    "                                eps = 1e-8\n",
    "    \n",
    "                                # Compute cosine similarity matrix\n",
    "                                cos_sim_matrix = F.cosine_similarity(\n",
    "                                    features.unsqueeze(1),  # [N, 1, D]\n",
    "                                    features.unsqueeze(0),  # [1, N, D]\n",
    "                                    dim=2,\n",
    "                                    eps=eps\n",
    "                                )  # shape [N, N]\n",
    "    \n",
    "                                # Remove self-similarity bias\n",
    "                                num = features.size(0)\n",
    "                                mask = (1 - torch.eye(num, device=features.device))\n",
    "                                cos_sim_matrix = cos_sim_matrix * mask\n",
    "    \n",
    "                                # ---- Soft alignment (SSAL) ----\n",
    "                                # Step 1. Rescale cosine to [0,1]\n",
    "                                cos_sim_matrix = (cos_sim_matrix + 1) / 2\n",
    "    \n",
    "                                # Step 2. Compute temperature-scaled soft distribution\n",
    "                                tau = 0.07  # you can tune in [0.03â€“0.1]\n",
    "                                sim_soft = torch.exp(cos_sim_matrix / tau)\n",
    "                                prob_matrix = sim_soft / (sim_soft.sum(dim=1, keepdim=True) + eps)\n",
    "    \n",
    "                                # Step 3. Soft Semantic Alignment Loss\n",
    "                                loss_sim = ((1 - cos_sim_matrix) * prob_matrix).sum(dim=1).mean()\n",
    "    \n",
    "                            else:\n",
    "                                loss_sim = torch.tensor(0.0, device=embeddings.device)\n",
    "    \n",
    "                            soft_mask = (soft_mask > 0).float()\n",
    "                            # print(soft_mask.mean(), gt_masks_new[i].mean())\n",
    "    \n",
    "                            # Apply entropy mask to losses\n",
    "                            loss_focal += focal_loss(pred_mask, soft_mask)  #, entropy_mask=entropy_mask\n",
    "                            loss_dice += dice_loss(pred_mask, soft_mask)   #, entropy_mask=entropy_mask\n",
    "                            batch_iou = calc_iou(pred_mask, soft_mask)\n",
    "                            loss_iou += F.mse_loss(iou_prediction, batch_iou, reduction='sum') / num_masks\n",
    "    \n",
    "                            if len(embedding_queue) > window_size:\n",
    "                                embedding_queue.pop(0)\n",
    "                \n",
    "                    del  pred_masks, iou_predictions, entropy_maps, preds\n",
    "                \n",
    "                    loss_total =  20 * loss_focal +  loss_dice  + loss_iou + 0.1*loss_sim #+ loss_iou  +  +\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                break \n",
    "    \n",
    "                    \n",
    "    \n",
    "                #     fabric.backward(loss_total)\n",
    "    \n",
    "                #     optimizer.step()\n",
    "                #     scheduler.step()\n",
    "                #     optimizer.zero_grad()\n",
    "                #     torch.cuda.empty_cache()\n",
    "                #     del  prompts, soft_masks\n",
    "    \n",
    "                #     batch_time.update(time.time() - end)\n",
    "                #     end = time.time()\n",
    "    \n",
    "                #     focal_losses.update(loss_focal.item(), batch_size)\n",
    "                #     dice_losses.update(loss_dice.item(), batch_size)\n",
    "                #     iou_losses.update(loss_iou.item(), batch_size)\n",
    "                #     total_losses.update(loss_total.item(), batch_size)\n",
    "                #     sim_losses.update(loss_sim.item(), batch_size)\n",
    "                \n",
    "                #     del loss_dice, loss_iou, loss_focal\n",
    "    \n",
    "                # if (iter+1) % match_interval==0:\n",
    "                #     fabric.print(\n",
    "                #         f\"Epoch [{epoch}] Iter [{iter + 1}/{len(train_dataloader)}] \"\n",
    "                #         f\"| Focal {focal_losses.avg:.4f} | Dice {dice_losses.avg:.4f} | \"\n",
    "                #         f\"IoU {iou_losses.avg:.4f} | Sim_loss {sim_losses.avg:.4f} | Total {total_losses.avg:.4f}\"\n",
    "                #     )\n",
    "                # if (iter+1) % 700 == 0:\n",
    "                #     val_iou, _ = validate(fabric, cfg, model, val_dataloader, cfg.name, epoch)\n",
    "    \n",
    "                #     status = \"\"\n",
    "                #     if val_iou > best_iou:  #best_iou\n",
    "                #         best_iou = val_iou\n",
    "                #         best_state = copy.deepcopy(model.state_dict())\n",
    "                #         torch.save(best_state, os.path.join(cfg.out_dir, \"save\", \"best_model.pth\"))\n",
    "                #         status = \"Improved â†’ Model Saved\"\n",
    "                #         no_improve_count = 0\n",
    "                #     else:\n",
    "                #         model.load_state_dict(best_state)\n",
    "                #         no_improve_count += 1\n",
    "                #         status = f\"Rollback ({no_improve_count})\"\n",
    "    \n",
    "                #     # Write log entry\n",
    "                #     with open(csv_path, \"a\", newline=\"\") as f:\n",
    "                #         writer = csv.writer(f)\n",
    "                #         writer.writerow([epoch, iter + 1, val_iou, best_iou, status])\n",
    "    \n",
    "                #     fabric.print(f\"Validation IoU={val_iou:.4f} | Best={best_iou:.4f} | {status}\")\n",
    "    \n",
    "                #     # Stop if model fails to stabilize\n",
    "                #     if no_improve_count >= max_patience:\n",
    "                #         fabric.print(f\"Training stopped early after {no_improve_count} failed rollbacks.\")\n",
    "                #         return\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f931a2af-e639-4955-8841-abcd3f6c57d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Configuration Loading and Launch\n",
    "\n",
    "# %% [code]\n",
    "# Example: set arguments manually here\n",
    "# Replace with your config module path, e.g. \"configs.default_config\"\n",
    "import importlib\n",
    "\n",
    "CFG_MODULE = \"configs.config_nwpu\"\n",
    "cfg_module = importlib.import_module(CFG_MODULE)\n",
    "cfg = cfg_module.cfg\n",
    "\n",
    "# Manually merge updates if needed\n",
    "cfg.out_dir = \"./outputs\"\n",
    "cfg.resume = False\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea3a42eb-f945-4983-a4c8-09ac369b8f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.26s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "gpu_ids = [str(i) for i in range(torch.cuda.device_count())]\n",
    "num_devices = len(gpu_ids)\n",
    "fabric = L.Fabric(accelerator=\"auto\",\n",
    "                  devices=num_devices,\n",
    "                  strategy=\"auto\",\n",
    "                  loggers=[TensorBoardLogger(cfg.out_dir)])\n",
    "fabric.launch()\n",
    "fabric.seed_everything(1337 + fabric.global_rank)\n",
    "\n",
    "if fabric.global_rank == 0:\n",
    "    os.makedirs(os.path.join(cfg.out_dir, \"save\"), exist_ok=True)\n",
    "    create_csv(os.path.join(cfg.out_dir, \"metrics.csv\"), csv_head=cfg.csv_keys)\n",
    "\n",
    "with fabric.device:\n",
    "    model = Model(cfg)\n",
    "    model.setup()\n",
    "    \n",
    "\n",
    "load_datasets = call_load_dataset(cfg)\n",
    "train_data, val_data, pt_data = load_datasets(cfg, img_size=1024, return_pt = True)\n",
    "train_data = fabric._setup_dataloader(train_data)\n",
    "val_data = fabric._setup_dataloader(val_data)\n",
    "pt_data = fabric._setup_dataloader(pt_data)\n",
    "optimizer, scheduler = configure_opt(cfg, model)\n",
    "model, optimizer = fabric.setup(model, optimizer)\n",
    "\n",
    "\n",
    "\n",
    "auto_ckpt = None\n",
    "\n",
    "\n",
    "if auto_ckpt is not None:\n",
    "    full_checkpoint = fabric.load(auto_ckpt)\n",
    "\n",
    "    if isinstance(full_checkpoint, dict) and \"model\" in full_checkpoint:\n",
    "        model.load_state_dict(full_checkpoint[\"model\"])\n",
    "        if \"optimizer\" in full_checkpoint:\n",
    "            optimizer.load_state_dict(full_checkpoint[\"optimizer\"])\n",
    "    else:\n",
    "        model.load_state_dict(full_checkpoint)\n",
    "    loaded = True\n",
    "    fabric.print(f\"Resumed from explicit checkpoint: {cfg.model.ckpt}\")\n",
    "    # state = torch.load(auto_ckpt, map_location=fabric.device)\n",
    "    # if isinstance(state, dict) and \"model\" in state:\n",
    "    #     model.load_state_dict(state[\"model\"])\n",
    "    #     if \"optimizer\" in state:\n",
    "    #         optimizer.load_state_dict(state[\"optimizer\"])\n",
    "    # else:\n",
    "    #     model.load_state_dict(state)\n",
    "    # fabric.print(f\"Auto-resumed from: {auto_ckpt}\")\n",
    "init_iou = 0.67\n",
    "# print('-'*100)\n",
    "# print('\\033[92mDirect test on the original SAM.\\033[0m') \n",
    "# init_iou, _, = validate(fabric, cfg, model, val_data, name=cfg.name, epoch=0)\n",
    "# print('-'*100)\n",
    "# del _     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85837094-231d-4781-81b3-80d0561bc36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with rollback enabled. Logging to: ./outputs/training_log.csv\n",
      "374\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 5.68 GiB of which 538.88 MiB is free. Process 2768 has 6.28 MiB memory in use. Including non-PyTorch memory, this process has 5.12 GiB memory in use. Of the allocated memory 4.13 GiB is allocated by PyTorch, and 883.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_sam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfabric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_iou\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 128\u001b[0m, in \u001b[0;36mtrain_sam\u001b[0;34m(cfg, fabric, model, optimizer, scheduler, train_dataloader, val_dataloader, init_iou, vis)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    125\u001b[0m     embeddings, soft_masks, _, _ \u001b[38;5;241m=\u001b[39m model(images_weak, bboxes\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m--> 128\u001b[0m _, pred_masks, iou_predictions, _\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_strong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_prompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# print(preds[0].shape)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# print(soft_masks[0].shape)\u001b[39;00m\n\u001b[1;32m    134\u001b[0m  \u001b[38;5;66;03m# Visualization\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/lightning/fabric/wrappers.py:110\u001b[0m, in \u001b[0;36m_FabricModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_precision\u001b[38;5;241m.\u001b[39mconvert_input((args, kwargs))\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_precision\u001b[38;5;241m.\u001b[39mforward_context():\n\u001b[0;32m--> 110\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_precision\u001b[38;5;241m.\u001b[39mconvert_output(output)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/github/SAM_RSI/utils/model.py:93\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, images, prompts)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, prompts):\n\u001b[1;32m     91\u001b[0m     _, _, H, W \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;66;03m#[n, 3, 1024, 1024]\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     image_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     pred_masks, ious, res_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode((H, W), prompts, image_embeddings)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_embeddings, pred_masks, ious, res_masks\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/github/SAM_RSI/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/github/SAM_RSI/segment_anything/modeling/image_encoder.py:174\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    172\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 174\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/github/SAM_RSI/segment_anything/modeling/image_encoder.py:234\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 234\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    237\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/github/SAM_RSI/segment_anything/modeling/image_encoder.py:358\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    354\u001b[0m rel_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhwc,hkc->bhwk\u001b[39m\u001b[38;5;124m\"\u001b[39m, r_q, Rh)\n\u001b[1;32m    355\u001b[0m rel_w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhwc,wkc->bhwk\u001b[39m\u001b[38;5;124m\"\u001b[39m, r_q, Rw)\n\u001b[1;32m    357\u001b[0m attn \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 358\u001b[0m     \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_w\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrel_h\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrel_w\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    359\u001b[0m )\u001b[38;5;241m.\u001b[39mview(B, q_h \u001b[38;5;241m*\u001b[39m q_w, k_h \u001b[38;5;241m*\u001b[39m k_w)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 5.68 GiB of which 538.88 MiB is free. Process 2768 has 6.28 MiB memory in use. Including non-PyTorch memory, this process has 5.12 GiB memory in use. Of the allocated memory 4.13 GiB is allocated by PyTorch, and 883.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_sam(cfg, fabric, model, optimizer, scheduler, train_data, val_data, init_iou, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6316fd58-488e-412e-9f51-ffcfd196b1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbe474-ec55-4a87-88b4-73a5fca4b126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78514ac-5be4-485a-8d15-22f936b1a92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207e5aa-4198-4126-8b0f-cd91593d4aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740e85d4-5669-4b85-bf6e-b7edd7cb6880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdabbb6-0db0-4cd1-88e3-35c6c0c48115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e71d266-7a26-4ca3-aa4b-23dfb6f981da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa6aa6-689a-4b8f-a871-e71e43f902ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f68291-6ae9-4e9a-be37-75ebbd44f57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0631ab-d878-48b3-9374-7249a22b96ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac6d54-a0d9-4a48-8bdf-d08929517f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bcb978-f670-4a17-83e5-5cb1743478f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65bd4a-32e5-4bed-ab13-d5daba4c818a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8515677-fc44-47cd-b80b-a8ec8da98bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6e1a4-14aa-4237-8036-a0db0cbfffee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e405fd-4f46-479b-9967-f60c7f14150d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf444b-577e-4699-84b1-f6855ab29993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33b5c87-d442-4cd8-b9b6-a893fde2a9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c745d-2514-4b97-a00d-b7e3f6fda48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dadb49b-35cc-449b-bc73-19dcc98b1737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb33455-e207-4330-b17c-4b962df61bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82e71d-29e7-43c2-8adc-043c97840757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e7e85597-5593-4cc5-9bef-08fbce3b402c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 62.9978, 156.2468, 146.6130, 251.3434],\n",
      "        [408.9128, 682.1429, 492.5280, 773.8023],\n",
      "        [497.1096, 626.0014, 553.2349, 686.7258],\n",
      "        [326.4430, 718.8066, 384.8591, 786.4055],\n",
      "        [802.9351, 800.1544, 883.1141, 862.0245],\n",
      "        [878.5324, 744.0130, 952.9843, 804.7374]], device='cuda:0',\n",
      "       dtype=torch.float64),)\n",
      "(tensor([[205.0304, 403.1925, 620.8504, 556.3290]], device='cuda:0',\n",
      "       dtype=torch.float64),)\n",
      "(tensor([[280.8439, 395.4326, 622.1772, 513.1195],\n",
      "        [278.6835, 319.8539, 357.5359, 379.2372],\n",
      "        [ 50.7679, 380.3169, 149.0633, 441.8596],\n",
      "        [629.7384, 273.4269, 729.1139, 341.4478],\n",
      "        [772.3207, 429.9829, 858.7342, 486.1271]], device='cuda:0',\n",
      "       dtype=torch.float64),)\n",
      "(tensor([[465.7175, 461.3976, 626.2599, 569.8313]], device='cuda:0',\n",
      "       dtype=torch.float64),)\n"
     ]
    }
   ],
   "source": [
    "for iter, data in enumerate(train_data):\n",
    "    images_weak, images_strong, bboxes, gt_masks, img_paths= data\n",
    "    print(bboxes)\n",
    "    if iter>2:\n",
    "        break\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81265e2c-95cf-4108-86ba-516022e5af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, _, = validate(fabric, cfg, model, val_data, name=cfg.name, epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d70e5a2-a8d3-436f-ae8f-632a8866bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_calibration(cfg, entrop_map, prompts, point_status):\n",
    "    point_list = []\n",
    "    point_labels_list = []\n",
    "    num_points = cfg.num_points\n",
    "\n",
    "    for m in range(len(entrop_map)):\n",
    "        point_coords = prompts[0][0][m][:].unsqueeze(0)\n",
    "        point_coords_lab = prompts[0][1][m][:].unsqueeze(0)\n",
    "\n",
    "        # Find high-entropy location\n",
    "        max_idx = torch.argmax(entrop_map[m])\n",
    "        y = max_idx // entrop_map[m].shape[1]\n",
    "        x = max_idx % entrop_map[m].shape[1]\n",
    "        neg_point_coords = torch.tensor([[x.item(), y.item()]], device=point_coords.device).unsqueeze(0)\n",
    "\n",
    "\n",
    "        # Combine positive and negative points\n",
    "        point_coords_all = torch.cat((point_coords, neg_point_coords), dim=1)\n",
    "        \n",
    "        # Append a new label (1) to the label tensor\n",
    "        point_labels_all = torch.cat(\n",
    "            (point_coords_lab, torch.tensor([[point_status]], device=point_coords.device, dtype=point_coords_lab.dtype)),\n",
    "            dim=1\n",
    "        )\n",
    "        \n",
    "        point_list.append(point_coords_all)\n",
    "        point_labels_list.append(point_labels_all)\n",
    "\n",
    "\n",
    "\n",
    "    point_ = torch.cat(point_list).squeeze(1)\n",
    "    point_labels_ = torch.cat(point_labels_list)\n",
    "    new_prompts = [(point_, point_labels_)]\n",
    "    return new_prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79477525-9853-4ad9-a975-e72a582627d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def entropy_map_calculate(p, eps=1e-8):\n",
    "    # Clamp to avoid log(0)\n",
    "    p = torch.clamp(p, eps, 1 - eps)\n",
    "    \n",
    "    # Compute binary entropy\n",
    "    entropy_map = - (p * torch.log(p) + (1 - p) * torch.log(1 - p))\n",
    "    \n",
    "    # Normalize to 0â€“1 (since max entropy = log(2))\n",
    "    entropy_map = entropy_map / torch.log(torch.tensor(2.0))\n",
    "    \n",
    "    return entropy_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96c39a90-63d8-45fa-8876-654e1a5410a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_forward(img_tensor, prompt):\n",
    "    with torch.no_grad():\n",
    "        _, masks_pred, _, _ = model(img_tensor, prompt)\n",
    "    entropy_maps = []\n",
    "    pred_ins = []\n",
    "    for i, mask_p in enumerate( masks_pred[0]):\n",
    "\n",
    "        p = mask_p.clamp(1e-6, 1 - 1e-6)\n",
    "        if p.ndim == 2:\n",
    "            p = p.unsqueeze(0)\n",
    "\n",
    "        entropy_map = entropy_map_calculate(p)\n",
    "        entropy_maps.append(entropy_map)\n",
    "        pred_ins.append(p)\n",
    "\n",
    "    return entropy_maps, pred_ins\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3c592e9c-6861-4a20-a5ac-e39ae2db12ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def edge_corner_score(x, y, x_c, y_c, w, h, gamma=1.0):\n",
    "    dx = 2 * torch.abs(x - x_c) / w\n",
    "    dy = 2 * torch.abs(y - y_c) / h\n",
    "    dx = torch.clamp(dx, 0, 1)\n",
    "    dy = torch.clamp(dy, 0, 1)\n",
    "    score = (dx + dy - dx * dy) ** gamma\n",
    "    return score\n",
    "\n",
    "# Rectangle parameters\n",
    "w, h = 100, 500\n",
    "x_c, y_c = w / 2, h / 2\n",
    "\n",
    "# Create grid of coordinates\n",
    "x = torch.linspace(0, w, 400)\n",
    "y = torch.linspace(0, h, 300)\n",
    "xx, yy = torch.meshgrid(x, y, indexing=\"xy\")\n",
    "\n",
    "# Compute edge/corner score over the grid\n",
    "score_map = edge_corner_score(xx, yy, x_c, y_c, w, h, gamma=0.7)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.imshow(score_map.numpy(), origin=\"lower\", cmap=\"plasma\", extent=[0, w, 0, h])\n",
    "plt.colorbar(label=\"Edge/Corner Score\")\n",
    "plt.title(\"Edge + Corner Proximity Heatmap\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.scatter([x_c], [y_c], c=\"white\", s=30, label=\"Center\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "df225794-7276-4b4a-9248-963f315c29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def edge_corner_score(x, y, x_c, y_c, w, h, gamma=0.7):\n",
    "    dx = 2 * torch.abs(x - x_c) / w\n",
    "    dy = 2 * torch.abs(y - y_c) / h\n",
    "    dx = torch.clamp(dx, 0, 1)\n",
    "    dy = torch.clamp(dy, 0, 1)\n",
    "    # high on edges + corners, low at center\n",
    "    score = (dx + dy - dx * dy) ** gamma\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "18bd3bfc-2d18-4c57-bc0f-cb0f4ed6e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def adjust_bbox_by_gradient(prob_map, bbox, max_iter=50, step=1, gain_thresh=0.001, device='cpu'):\n",
    "    \"\"\"\n",
    "    Expands or shifts a bounding box toward the segmentation region\n",
    "    guided by gradients in the probability map.\n",
    "    \"\"\"\n",
    "    if isinstance(prob_map, np.ndarray):\n",
    "        prob_map = torch.from_numpy(prob_map)\n",
    "    prob_map = prob_map.float().to(device)\n",
    "    H, W = prob_map.shape\n",
    "\n",
    "    bbox = torch.tensor(bbox, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Compute gradients once\n",
    "    gy, gx = torch.gradient(prob_map)\n",
    "    grad_mag = torch.sqrt(gx**2 + gy**2)\n",
    "\n",
    "    def clamp_box(b):\n",
    "        x1, y1, x2, y2 = b\n",
    "        return torch.tensor([\n",
    "            torch.clamp(x1, 0, W-1),\n",
    "            torch.clamp(y1, 0, H-1),\n",
    "            torch.clamp(x2, 1, W),\n",
    "            torch.clamp(y2, 1, H)\n",
    "        ], device=device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x1, y1, x2, y2 = bbox.int()\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(W-1, x2), min(H-1, y2)\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            break\n",
    "\n",
    "        # Measure edge gradient means\n",
    "        top_grad = grad_mag[y1, x1:x2].mean()\n",
    "        bottom_grad = grad_mag[y2-1, x1:x2].mean()\n",
    "        left_grad = grad_mag[y1:y2, x1].mean()\n",
    "        right_grad = grad_mag[y1:y2, x2-1].mean()\n",
    "\n",
    "        edges = {\n",
    "            'up': top_grad,\n",
    "            'down': bottom_grad,\n",
    "            'left': left_grad,\n",
    "            'right': right_grad\n",
    "        }\n",
    "\n",
    "        best_edge = max(edges, key=edges.get)\n",
    "        best_grad = edges[best_edge].item()\n",
    "\n",
    "        if best_grad < gain_thresh:\n",
    "            break\n",
    "\n",
    "        # Move or expand the chosen edge outward\n",
    "        if best_edge == 'up':\n",
    "            bbox[1] -= step\n",
    "        elif best_edge == 'down':\n",
    "            bbox[3] += step\n",
    "        elif best_edge == 'left':\n",
    "            bbox[0] -= step\n",
    "        elif best_edge == 'right':\n",
    "            bbox[2] += step\n",
    "\n",
    "        bbox = clamp_box(bbox)\n",
    "\n",
    "    # Final mask\n",
    "    final_mask = torch.zeros_like(prob_map)\n",
    "    x1, y1, x2, y2 = bbox.int()\n",
    "    final_mask[y1:y2, x1:x2] = 1.0\n",
    "\n",
    "    return bbox.cpu().tolist(), final_mask.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "2d13625b-f65c-48ec-887c-275744a4bf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0012\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0018\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0033\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0094\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0076\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0313\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0117\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0180\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0386\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0698\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0168\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0145\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0145\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0145\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0145\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0145\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0145\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0145\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 1 mean entropy inside bbox: 0.0145\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Saved entropy_sorted/1_0_entropy_heatmap.png\n",
      "Saved entropy_sorted/1_0_mask_clean_box.png\n",
      "Mask 1 mean entropy inside bbox: 0.0000\n",
      "Saved entropy_sorted/1_1_entropy_heatmap.png\n",
      "Saved entropy_sorted/1_1_mask_clean_box.png\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Saved entropy_sorted/2_0_entropy_heatmap.png\n",
      "Saved entropy_sorted/2_0_mask_clean_box.png\n",
      "Mask 0 mean entropy inside bbox: 0.0001\n",
      "Mask 0 mean entropy inside bbox: 0.0003\n",
      "Mask 0 mean entropy inside bbox: 0.0011\n",
      "Mask 0 mean entropy inside bbox: 0.0007\n",
      "Mask 0 mean entropy inside bbox: 0.0003\n",
      "Mask 0 mean entropy inside bbox: 0.0002\n",
      "Mask 0 mean entropy inside bbox: 0.0003\n",
      "Mask 0 mean entropy inside bbox: 0.0005\n",
      "Mask 0 mean entropy inside bbox: 0.0012\n",
      "Mask 0 mean entropy inside bbox: 0.0038\n",
      "Mask 0 mean entropy inside bbox: 0.0022\n",
      "Mask 0 mean entropy inside bbox: 0.0024\n",
      "Mask 0 mean entropy inside bbox: 0.0021\n",
      "Mask 0 mean entropy inside bbox: 0.0020\n",
      "Mask 0 mean entropy inside bbox: 0.0020\n",
      "Mask 0 mean entropy inside bbox: 0.0020\n",
      "Mask 0 mean entropy inside bbox: 0.0020\n",
      "Mask 0 mean entropy inside bbox: 0.0020\n",
      "Mask 0 mean entropy inside bbox: 0.0020\n",
      "Mask 0 mean entropy inside bbox: 0.0020\n",
      "Saved entropy_sorted/3_0_entropy_heatmap.png\n",
      "Saved entropy_sorted/3_0_mask_clean_box.png\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Saved entropy_sorted/4_0_entropy_heatmap.png\n",
      "Saved entropy_sorted/4_0_mask_clean_box.png\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0000\n",
      "Mask 0 mean entropy inside bbox: 0.0013\n",
      "Mask 0 mean entropy inside bbox: 0.0007\n",
      "Mask 0 mean entropy inside bbox: 0.0008\n",
      "Mask 0 mean entropy inside bbox: 0.0010\n",
      "Mask 0 mean entropy inside bbox: 0.0005\n",
      "Saved entropy_sorted/5_0_entropy_heatmap.png\n",
      "Saved entropy_sorted/5_0_mask_clean_box.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "save_dir = \"entropy_sorted\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "W, H = 1024, 1024\n",
    "\n",
    "for rank, (entropy_scalar, img_path, render) in enumerate(collected, start=1):\n",
    "    img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "    # ---- Convert and move to device\n",
    "    img_np = render['real']        # numpy HxWx3\n",
    "    img_tensor = torch.from_numpy(img_np).permute(2,0,1).float() / 255.0\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(fabric.device)\n",
    "\n",
    "    prompt_main = render['prompt']\n",
    "    # ---- Forward pass to get entropy + predictions\n",
    "    # entropy_maps, preds = process_forward(img_tensor, prompt_main)\n",
    "\n",
    "    # print(prompt_main[0][0].shape)\n",
    "    # pred_stack = torch.stack(preds, dim=0)\n",
    "\n",
    "    # # Convert to binary masks (e.g., threshold 0.99 as you do)\n",
    "    # pred_binary = (pred_stack > 0.99).float() \n",
    "    # # Count overlaps\n",
    "    # overlap_count = pred_binary.sum(dim=0)  # (1,1024,1024)\n",
    "    \n",
    "    # # Optional: extract overlapping region mask (for debugging)\n",
    "    # overlap_map = (overlap_count > 1).float()\n",
    "    # invert_overlap_map = 1.0 - overlap_map\n",
    "    \n",
    "    # # Convert to uint8 for visualization/saving\n",
    "    # # non_overlap_vis = (non_overlap_map[0].cpu().numpy() * 255).astype(np.uint8)\n",
    "    \n",
    "\n",
    "    # # # ---- Save the original image once\n",
    "    img_save_path = os.path.join(save_dir, f\"{rank}.jpg\")\n",
    "    cv2.imwrite(img_save_path, cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # scores = []\n",
    "    # # ---- Loop over instances (each mask + entropy)\n",
    "    # for i, (entr_map, pred) in enumerate(zip(entropy_maps, preds)):\n",
    "    #     # # Normalize entropy 0â€“1\n",
    "    #     # entr_norm = (entr_map - entr_map.min()) / (entr_map.max() - entr_map.min() + 1e-8)\n",
    "\n",
    "    #     # # Convert to uint8\n",
    "    #     # entr_vis = (entr_norm[0].cpu().numpy() * 255).astype(np.uint8)\n",
    "    #     pred = (pred[0]>0.99) \n",
    "    #     pred_w_overlap = pred * invert_overlap_map[0]\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    #     # Find where mask == 1\n",
    "    #     ys, xs = torch.where(pred_w_overlap > 0.5)\n",
    "    #     if len(xs) > 0 and len(ys) > 0:\n",
    "    #         x_min, x_max = xs.min().item(), xs.max().item()\n",
    "    #         y_min, y_max = ys.min().item(), ys.max().item()\n",
    "    #         h, w = y_max - y_min, x_max - x_min\n",
    "    #         print(f\"Bounding box: ({x_min}, {y_min}) â†’ ({x_max}, {y_max})\") \n",
    "    #         cx = (x_min + x_max) / 2.0\n",
    "    #         cy = (y_min + y_max) / 2.0\n",
    "    #         # # Optional: draw rectangle\n",
    "    #         # mask_vis = (pred_w_overlap.cpu().numpy() * 255).astype(np.uint8)\n",
    "    #         # mask_rgb = cv2.cvtColor(mask_vis, cv2.COLOR_GRAY2BGR)\n",
    "    #         # cv2.rectangle(mask_rgb, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "    #         # pred_path = os.path.join(save_dir, f\"{rank:04d}_{img_name}_{i}mask_with_box.png\")\n",
    "    #         # cv2.imwrite(pred_path, mask_rgb)\n",
    "    #     else:\n",
    "    #         print(\"No 1s found in mask\")\n",
    "    #     # Make sure both tensors are on same device\n",
    "    #     point_ref = torch.tensor([cx, cy], dtype=torch.float32, device=prompt_main[0][0][i].device)\n",
    "    #     score = edge_corner_score(prompt_main[0][0][i][0][0].item(), prompt_main[0][0][i][0][1].item(), point_ref[0], point_ref[1], w, h)\n",
    "    #     scores.append(score)\n",
    "    #     # point_ref = torch.tensor([cx, cy], dtype=torch.float32, device=prompt_main[0][0][i].device)\n",
    "    #     # dist = torch.dist(prompt_main[0][0][i], point_ref)\n",
    "        \n",
    "    #     # pred_w_overlap_vis = (pred_w_overlap.cpu().numpy()* 255).astype(np.uint8)\n",
    "    \n",
    "\n",
    "    #     # # Save prediction mask\n",
    "    #     # pred_path = os.path.join(save_dir, f\"{rank:04d}_{img_name}_{i}_pred.png\")\n",
    "    #     # cv2.imwrite(pred_path, pred_w_overlap_vis)\n",
    "    # print(f\"Saved image + {len(entropy_maps)} instance maps for {img_name}\")\n",
    "\n",
    "\n",
    "    x_offset = 16\n",
    "    y_offset = 16\n",
    "    refine_iter = 20\n",
    "    for refine in range(refine_iter):\n",
    "\n",
    "        if refine == 0:\n",
    "            bboxes = []\n",
    "            for j in range(len(prompt_main[0][0])):\n",
    "                x = prompt_main[0][0][j][0][0]\n",
    "                y = prompt_main[0][0][j][0][1]\n",
    "    \n",
    "                bboxes.append(torch.tensor([x-x_offset, y - y_offset, x + x_offset, y + y_offset], dtype=torch.float32))\n",
    "\n",
    "            bboxes = torch.stack(bboxes)\n",
    "        with torch.no_grad():\n",
    "            _, masks_pred, _, _ = model(img_tensor, bboxes.unsqueeze(0))\n",
    "\n",
    "        # masks_pred[0] shape -> [num_boxes, H, W]\n",
    "        pred_masks = masks_pred[0].cpu()  \n",
    "        bbox_new = []\n",
    "        for i, mask in enumerate(pred_masks):\n",
    "            # Threshold mask to binary: 0 or 255\n",
    "            mask_bin = (mask > 0.5).numpy().astype(np.uint8) * 255  # clean mask\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # -------------------------\n",
    "            # Compute and save entropy heatmap\n",
    "            # -------------------------\n",
    "            # Convert mask to probability (0-1)\n",
    "            mask_prob = mask.numpy()\n",
    "            \n",
    "            # Avoid log(0) by clamping probabilities\n",
    "            eps = 1e-6\n",
    "            mask_prob = np.clip(mask_prob, eps, 1 - eps)\n",
    "            # Compute entropy per pixel\n",
    "            entropy_map = - (mask_prob * np.log(mask_prob) + (1 - mask_prob) * np.log(1 - mask_prob))\n",
    "            \n",
    "            # Normalize entropy to 0-255 for visualization\n",
    "            entropy_norm = ((entropy_map - entropy_map.min()) / (entropy_map.max() - entropy_map.min()) * 255).astype(np.uint8)\n",
    "            \n",
    "            # Crop the entropy map to the bounding box\n",
    "            entropy_roi = entropy_map[y1:y2, x1:x2]\n",
    "            \n",
    "            # Compute mean entropy inside the rectangle\n",
    "            mean_entropy = entropy_roi.mean()\n",
    "            \n",
    "            print(f\"Mask {i} mean entropy inside bbox: {mean_entropy:.4f}\")\n",
    "                    \n",
    "\n",
    "      \n",
    "            prob_map = mask.numpy()\n",
    "            opt_box, _ = adjust_bbox_by_gradient(prob_map, bboxes[i].int().tolist())\n",
    "            opt_box = list(map(int, opt_box))\n",
    "          \n",
    "            bbox_new.append(torch.tensor(opt_box, dtype=torch.float32))\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            if refine == (refine_iter-1):\n",
    "                \n",
    "                # Convert to 3-channel for colored box\n",
    "                mask_color = cv2.cvtColor(mask_bin, cv2.COLOR_GRAY2BGR)\n",
    "                \n",
    "                # Draw bounding box\n",
    "                x1, y1, x2, y2 = bboxes[i].int().tolist()\n",
    "                cv2.rectangle(mask_color, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "    \n",
    "                # Apply a colormap for better visualization\n",
    "                entropy_color = cv2.applyColorMap(entropy_norm, cv2.COLORMAP_JET)\n",
    "                # Draw bounding box on entropy map as well\n",
    "                cv2.rectangle(entropy_color, (x1, y1), (x2, y2), color=(255, 255, 255), thickness=2)\n",
    "                \n",
    "                # Save entropy heatmap\n",
    "                save_path_entropy = os.path.join(save_dir, f\"{rank}_{i}_entropy_heatmap.png\")\n",
    "                cv2.imwrite(save_path_entropy, entropy_color)\n",
    "                print(f\"Saved {save_path_entropy}\")\n",
    "    \n",
    "                # Draw bounding box\n",
    "                x1, y1, x2, y2 = opt_box\n",
    "                cv2.rectangle(mask_color, (x1, y1), (x2, y2), color=(0, 255, 255), thickness=2)\n",
    "                \n",
    "                \n",
    "    \n",
    "                # Save the clean mask with box\n",
    "                save_path = os.path.join(save_dir, f\"{rank}_{i}_mask_clean_box.png\")\n",
    "                cv2.imwrite(save_path, mask_color)\n",
    "                print(f\"Saved {save_path}\")\n",
    "        \n",
    "        bbox_new = torch.stack(bbox_new)\n",
    "        bboxes = bbox_new\n",
    "     \n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    # Stop early for testing\n",
    "    if rank >= 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8152bb-c4b6-49fe-bc8c-5f7c0d7b67da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e63697-036b-4adc-b308-7032bd381113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66827a0-c7b6-477e-ba19-eaa1dcf4058c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f221a73-a518-4ca1-8937-97dc22ec4ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13c127-fbb0-4fa3-b97f-a3136113262b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a3fa9-187d-49b7-a82d-fcc9b3dfa523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d738313-05d0-4e83-b730-f25aeacb3a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f268a-ac20-4137-ae18-07364c9505f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86362cd-940f-4751-a35b-1874690203da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e9e8bf-296b-41a3-9144-1ba16412490c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c57b8-6618-4417-b0e9-affcec932b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b823b846-cf4f-4780-bff5-a1ee4704edbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0407f-15d1-4e5e-96e2-c2e6137b6e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5202aadd-e917-4b44-8e2a-ef7dfd8ec566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db847e9f-7d77-4665-8a70-0056cc0d8fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb27bb-27ee-4525-b4aa-ca09627e7eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc5a99e-b911-45e9-afcc-a6a311b1f2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d104352-115c-4c57-9c0f-c046be931f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7d60d9-553a-4450-8ffe-b378b9ea517e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dc3acd-ff22-4fbe-b1de-36a05ec539f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b7147-d578-4edf-88fe-0daea9a9e2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c22afc-3053-4eee-86fd-78b2f31ce6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f06310b-0091-476f-ad9c-980799369adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c9f30-d4b9-4d36-93fd-b38c6433b0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac65916a-16ec-47ac-9187-1043cd38ad66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff142d69-d26f-4f37-9ecb-edf7c8e2b7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ddeec-5305-40ed-a314-4a4237f1ad3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3db350-c3be-43e0-a01d-09fdb95f13fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b9cdb-bafc-4b3b-bd04-d4f9157d62a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf4b887-6a7a-4d33-a252-909c5a45c2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb382e73-510f-440f-9522-96d091306df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82482b0a-98f5-4a2b-9033-9b211534dfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c237be62-e469-4655-8822-a0be202fa5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01301e45-b4d2-4615-b3f0-0be198502a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef8e4fb-5aa2-45e9-af88-ca0651d157c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
