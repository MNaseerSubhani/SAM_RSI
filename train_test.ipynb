{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d10da6-4073-40c8-8098-ccc759417552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tic/miniconda3/envs/cvpr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # SAM Training Notebook\n",
    "# Converted from Python script for interactive execution in Jupyter.\n",
    "# \n",
    "# This notebook preserves the original script logic and allows step-by-step debugging.\n",
    "\n",
    "# %% [code]\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "\n",
    "import lightning as L\n",
    "from lightning.fabric.loggers import TensorBoardLogger\n",
    "from lightning.fabric.fabric import _FabricOptimizer\n",
    "\n",
    "from box import Box\n",
    "from datasets import call_load_dataset\n",
    "from utils.model import Model\n",
    "from utils.eval_utils import AverageMeter, validate, get_prompts, calc_iou\n",
    "from utils.tools import copy_model, create_csv, reduce_instances\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d122bc-4b7f-4109-91d1-dc08774e6658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Training Function\n",
    "\n",
    "# %% [code]\n",
    "def train_sam(cfg: Box,\n",
    "              fabric: L.Fabric,\n",
    "              model: Model,\n",
    "              optimizer: _FabricOptimizer,\n",
    "              scheduler: _FabricOptimizer,\n",
    "              train_dataloader: DataLoader,\n",
    "              val_dataloader: DataLoader,\n",
    "              target_pts):\n",
    "    model.eval()\n",
    "\n",
    "    save_dir = \"entropy_sorted\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    collected = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(target_pts, desc='Computing per-sample entropy', ncols=100)):\n",
    "            imgs, boxes, masks, img_paths = batch\n",
    "            prompts = get_prompts(cfg, boxes, masks)\n",
    "            embeds, masks_pred, _, _ = model(imgs, prompts)\n",
    "\n",
    "            batch_size = imgs.shape[0]\n",
    "            for b in range(batch_size):\n",
    "                img_np = (imgs[b].permute(1,2,0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                p_b = masks_pred[b].clamp(1e-6, 1 - 1e-6)\n",
    "                if p_b.ndim == 2:\n",
    "                    p_b = p_b.unsqueeze(0)\n",
    "                gt_b = masks[b]\n",
    "                if gt_b.ndim == 2:\n",
    "                    gt_b = gt_b.unsqueeze(0)\n",
    "\n",
    "                entropy_scalar = 0\n",
    "                num_inst = p_b.shape[0]\n",
    "                for j in range(num_inst):\n",
    "                    p_inst = p_b[j]\n",
    "                    entropy_map_inst = - (p_inst * torch.log(p_inst) + (1 - p_inst) * torch.log(1 - p_inst))\n",
    "                    entropy_scalar += float(entropy_map_inst.mean().cpu().item())\n",
    "\n",
    "                entropy_scalar /= num_inst\n",
    "                render = {\n",
    "                    'real': img_np,\n",
    "                    'prompt': prompts\n",
    "                }\n",
    "                img_path = img_paths[b] if isinstance(img_paths, (list, tuple)) else img_paths\n",
    "                collected.append((entropy_scalar, img_path, render))\n",
    "\n",
    "    collected.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return collected\n",
    "\n",
    "    \n",
    "\n",
    "# %% [markdown]\n",
    "# ## Optimizer and Scheduler Configuration\n",
    "\n",
    "# %% [code]\n",
    "def configure_opt(cfg: Box, model: Model):\n",
    "    def lr_lambda(step):\n",
    "        if step < cfg.opt.warmup_steps:\n",
    "            return step / cfg.opt.warmup_steps\n",
    "        elif step < cfg.opt.steps[0]:\n",
    "            return 1.0\n",
    "        elif step < cfg.opt.steps[1]:\n",
    "            return 1 / cfg.opt.decay_factor\n",
    "        else:\n",
    "            return 1 / (cfg.opt.decay_factor**2)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.model.parameters(),\n",
    "                                 lr=cfg.opt.learning_rate,\n",
    "                                 weight_decay=cfg.opt.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f931a2af-e639-4955-8841-abcd3f6c57d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Configuration Loading and Launch\n",
    "\n",
    "# %% [code]\n",
    "# Example: set arguments manually here\n",
    "# Replace with your config module path, e.g. \"configs.default_config\"\n",
    "import importlib\n",
    "\n",
    "CFG_MODULE = \"configs.config_nwpu_\"\n",
    "cfg_module = importlib.import_module(CFG_MODULE)\n",
    "cfg = cfg_module.cfg\n",
    "\n",
    "# Manually merge updates if needed\n",
    "cfg.out_dir = \"./outputs\"\n",
    "cfg.resume = False\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea3a42eb-f945-4983-a4c8-09ac369b8f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing per-sample entropy: 100%|███████████████████████████████| 520/520 [15:20<00:00,  1.77s/it]\n"
     ]
    }
   ],
   "source": [
    "gpu_ids = [str(i) for i in range(torch.cuda.device_count())]\n",
    "fabric = L.Fabric(accelerator=\"auto\",\n",
    "                  devices=len(gpu_ids),\n",
    "                  strategy=\"auto\",\n",
    "                  loggers=[TensorBoardLogger(cfg.out_dir)])\n",
    "fabric.launch()\n",
    "fabric.seed_everything(1337 + fabric.global_rank)\n",
    "\n",
    "if fabric.global_rank == 0:\n",
    "    os.makedirs(os.path.join(cfg.out_dir, \"save\"), exist_ok=True)\n",
    "    create_csv(os.path.join(cfg.out_dir, \"metrics.csv\"), csv_head=cfg.csv_keys)\n",
    "\n",
    "with fabric.device:\n",
    "    model = Model(cfg)\n",
    "    model.setup()\n",
    "\n",
    "load_datasets = call_load_dataset(cfg)\n",
    "train_data, val_data, pt_data = load_datasets(cfg, img_size=1024, return_pt=True)\n",
    "train_data = fabric._setup_dataloader(train_data)\n",
    "val_data = fabric._setup_dataloader(val_data)\n",
    "pt_data = fabric._setup_dataloader(pt_data)\n",
    "\n",
    "optimizer, scheduler = configure_opt(cfg, model)\n",
    "model, optimizer = fabric.setup(model, optimizer)\n",
    "\n",
    "if cfg.resume and cfg.model.ckpt is not None:\n",
    "    full_checkpoint = fabric.load(cfg.model.ckpt)\n",
    "    model.load_state_dict(full_checkpoint[\"model\"])\n",
    "    optimizer.load_state_dict(full_checkpoint[\"optimizer\"])\n",
    "\n",
    "collected = train_sam(cfg, fabric, model, optimizer, scheduler, train_data, val_data, pt_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81265e2c-95cf-4108-86ba-516022e5af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, _, = validate(fabric, cfg, model, val_data, name=cfg.name, epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d70e5a2-a8d3-436f-ae8f-632a8866bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_calibration(cfg, entrop_map, prompts, point_status):\n",
    "    point_list = []\n",
    "    point_labels_list = []\n",
    "    num_points = cfg.num_points\n",
    "\n",
    "    for m in range(len(entrop_map)):\n",
    "        point_coords = prompts[0][0][m][:].unsqueeze(0)\n",
    "        point_coords_lab = prompts[0][1][m][:].unsqueeze(0)\n",
    "\n",
    "        # Find high-entropy location\n",
    "        max_idx = torch.argmax(entrop_map[m])\n",
    "        y = max_idx // entrop_map[m].shape[1]\n",
    "        x = max_idx % entrop_map[m].shape[1]\n",
    "        neg_point_coords = torch.tensor([[x.item(), y.item()]], device=point_coords.device).unsqueeze(0)\n",
    "\n",
    "\n",
    "        # Combine positive and negative points\n",
    "        point_coords_all = torch.cat((point_coords, neg_point_coords), dim=1)\n",
    "        \n",
    "        # Append a new label (1) to the label tensor\n",
    "        point_labels_all = torch.cat(\n",
    "            (point_coords_lab, torch.tensor([[point_status]], device=point_coords.device, dtype=point_coords_lab.dtype)),\n",
    "            dim=1\n",
    "        )\n",
    "        \n",
    "        point_list.append(point_coords_all)\n",
    "        point_labels_list.append(point_labels_all)\n",
    "\n",
    "\n",
    "\n",
    "    point_ = torch.cat(point_list).squeeze(1)\n",
    "    point_labels_ = torch.cat(point_labels_list)\n",
    "    new_prompts = [(point_, point_labels_)]\n",
    "    return new_prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79477525-9853-4ad9-a975-e72a582627d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_map_calculate(p):\n",
    "    entropy_map = - (p * torch.log(p) + (1 - p) * torch.log(1 - p))\n",
    "    entropy_map = entropy_map.max(dim=0)[0]\n",
    "\n",
    "    return entropy_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96c39a90-63d8-45fa-8876-654e1a5410a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_forward(img_tensor, prompt):\n",
    "    with torch.no_grad():\n",
    "        _, masks_pred, _, _ = model(img_tensor, prompt)\n",
    "    entropy_maps = []\n",
    "    pred_ins = []\n",
    "    for i, mask_p in enumerate( masks_pred[0]):\n",
    "\n",
    "        p = mask_p.clamp(1e-6, 1 - 1e-6)\n",
    "        if p.ndim == 2:\n",
    "            p = p.unsqueeze(0)\n",
    "\n",
    "        entropy_map = entropy_map_calculate(p)\n",
    "        entropy_maps.append(entropy_map)\n",
    "        pred_ins.append(p)\n",
    "\n",
    "    return entropy_maps, pred_ins\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb1d5e2f-1c24-43ff-b28c-e463c11e512f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 5.68 GiB of which 204.12 MiB is free. Including non-PyTorch memory, this process has 5.46 GiB memory in use. Of the allocated memory 4.21 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m img_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(fabric\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      6\u001b[0m prompt_main \u001b[38;5;241m=\u001b[39m render[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m entropy_maps, preds \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_main\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m prompt_1 \u001b[38;5;241m=\u001b[39m prompt_calibration(cfg, entropy_maps, prompt_main, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m entropy_maps_1, preds_1 \u001b[38;5;241m=\u001b[39m process_forward(img_tensor, prompt_1)\n",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m, in \u001b[0;36mprocess_forward\u001b[0;34m(img_tensor, prompt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_forward\u001b[39m(img_tensor, prompt):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m         _, masks_pred, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     entropy_maps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m     pred_ins \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/lightning/fabric/wrappers.py:110\u001b[0m, in \u001b[0;36m_FabricModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_precision\u001b[38;5;241m.\u001b[39mconvert_input((args, kwargs))\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_precision\u001b[38;5;241m.\u001b[39mforward_context():\n\u001b[0;32m--> 110\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_precision\u001b[38;5;241m.\u001b[39mconvert_output(output)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/github/SAM_RSI/utils/model.py:93\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, images, prompts)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, prompts):\n\u001b[1;32m     91\u001b[0m     _, _, H, W \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;66;03m#[n, 3, 1024, 1024]\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     image_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     pred_masks, ious, res_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode((H, W), prompts, image_embeddings)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_embeddings, pred_masks, ious, res_masks\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/github/SAM_RSI/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/github/SAM_RSI/segment_anything/modeling/image_encoder.py:174\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    172\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 174\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cvpr/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/github/SAM_RSI/segment_anything/modeling/image_encoder.py:234\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 234\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    237\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/github/SAM_RSI/segment_anything/modeling/image_encoder.py:358\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    354\u001b[0m rel_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhwc,hkc->bhwk\u001b[39m\u001b[38;5;124m\"\u001b[39m, r_q, Rh)\n\u001b[1;32m    355\u001b[0m rel_w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhwc,wkc->bhwk\u001b[39m\u001b[38;5;124m\"\u001b[39m, r_q, Rw)\n\u001b[1;32m    357\u001b[0m attn \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 358\u001b[0m     \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_w\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrel_h\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m rel_w[:, :, :, \u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[1;32m    359\u001b[0m )\u001b[38;5;241m.\u001b[39mview(B, q_h \u001b[38;5;241m*\u001b[39m q_w, k_h \u001b[38;5;241m*\u001b[39m k_w)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 5.68 GiB of which 204.12 MiB is free. Including non-PyTorch memory, this process has 5.46 GiB memory in use. Of the allocated memory 4.21 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "save_dir=\"entropy_sorted\"\n",
    "for rank, (entropy_scalar, img_path, render) in enumerate(reversed(collected), start=1):\n",
    "    img_tensor = torch.from_numpy(render['real']).permute(2,0,1).float() / 255.0\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(fabric.device)\n",
    "\n",
    "    prompt_main = render['prompt']\n",
    "\n",
    "    entropy_maps, preds = process_forward(img_tensor, prompt_main)\n",
    "\n",
    "    \n",
    "\n",
    "    prompt_1 = prompt_calibration(cfg, entropy_maps, prompt_main, 1)\n",
    "    entropy_maps_1, preds_1 = process_forward(img_tensor, prompt_1)\n",
    "    entr_means_pos = [ent.mean().item() for ent in entropy_maps_1]\n",
    "\n",
    "    # Compute relative entropy = mean entropy / (number of 1s in prediction)\n",
    "    rel_entr_pos = []\n",
    "    for ent, pred in zip(entropy_maps_1, preds_1):\n",
    "        num_ones = (pred >0.5 ).sum().item()\n",
    "        rel_entr = ent.mean().item() / num_ones if num_ones > 0 else float('inf')\n",
    "        rel_entr_pos.append(rel_entr)\n",
    "\n",
    "\n",
    "    prompt_2 = prompt_calibration(cfg, entropy_maps, prompt_main, 0)\n",
    "    entropy_maps_2, preds_2 = process_forward(img_tensor, prompt_2)\n",
    "    entr_means_neg = [ent.mean().item() for ent in entropy_maps_2]\n",
    "\n",
    "    rel_entr_neg = []\n",
    "    for ent, pred in zip(entropy_maps_2, preds_2):\n",
    "        num_ones = (pred == 0.5).sum().item()\n",
    "        rel_entr = ent.mean().item() / num_ones if num_ones > 0 else float('inf')\n",
    "        rel_entr_neg.append(rel_entr)\n",
    "\n",
    "    pred_final = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(preds_1)):\n",
    "        if rel_entr_pos[i] < rel_entr_neg[i]:\n",
    "            pred_final.append(preds_1[i])\n",
    "        else:\n",
    "            pred_final.append(preds_2[i])\n",
    "        \n",
    "   \n",
    "    # for i,p in  enumerate(pred_final):\n",
    "    #     pred_mask = p.max(dim=0)[0]\n",
    "    #     pred_bin = (pred_mask > 0.5).to(torch.uint8).cpu().numpy() * 255\n",
    "\n",
    "\n",
    "    \n",
    "    #     base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    #     suffix = f\"{rank:05d}_{base}\"\n",
    "        \n",
    "    #     Image.fromarray(pred_bin).save(os.path.join(save_dir, f\"{suffix}{i}_pred.jpg\"))\n",
    "    #     # Image.fromarray(entropy_color).save(os.path.join(save_dir, f\"{suffix}{i}_en.jpg\"))\n",
    "\n",
    "    # Combine all pseudo-label predictions into one final mask\n",
    "    pred_stack = torch.stack([\n",
    "        (p.max(dim=0)[0] if p.ndim == 3 else p) for p in pred_final\n",
    "    ])\n",
    "    final_mask = pred_stack.mean(dim=0)  # or .max(dim=0)[0] for union\n",
    "    # Convert to binary pseudo label\n",
    "    final_bin = (final_mask > 0.06).to(torch.uint8).cpu().numpy() * 255\n",
    "\n",
    "    # Save combined pseudo-label\n",
    "    base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    suffix = f\"{rank:05d}_{base}\"\n",
    "    Image.fromarray(final_bin).save(os.path.join(save_dir, f\"{suffix}_pseudo_combined.jpg\"))\n",
    "\n",
    "\n",
    "    # Combine all pseudo-label predictions into one final mask\n",
    "    pred_first = torch.stack([\n",
    "        (p.max(dim=0)[0] if p.ndim == 3 else p) for p in preds\n",
    "    ])\n",
    "    final_mask = pred_first.mean(dim=0)  # or .max(dim=0)[0] for union\n",
    "    # Convert to binary pseudo label\n",
    "    final_bin_p = (final_mask > 0.06).to(torch.uint8).cpu().numpy() * 255\n",
    "\n",
    "    # Save combined pseudo-label\n",
    "    base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    suffix = f\"{rank:05d}_{base}\"\n",
    "    Image.fromarray(final_bin_p).save(os.path.join(save_dir, f\"{suffix}_pred_combined.jpg\"))\n",
    "    \n",
    "\n",
    "    \n",
    "    Image.fromarray(render['real']).save(os.path.join(save_dir, f\"{suffix}.jpg\"))\n",
    "\n",
    "\n",
    "    if rank>400:\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff142d69-d26f-4f37-9ecb-edf7c8e2b7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ddeec-5305-40ed-a314-4a4237f1ad3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3db350-c3be-43e0-a01d-09fdb95f13fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b9cdb-bafc-4b3b-bd04-d4f9157d62a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf4b887-6a7a-4d33-a252-909c5a45c2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd2f09-3257-4f9e-ae04-dfa0454b437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir=\"entropy_sorted\"\n",
    "\n",
    "for rank, (entropy_scalar, img_path, render) in enumerate(collected, start=1):\n",
    "    img_tensor = torch.from_numpy(render['real']).permute(2,0,1).float() / 255.0\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(fabric.device)\n",
    "\n",
    "    prompt_b = render['prompt']\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, masks_pred, _, _ = model(img_tensor, prompt_b)\n",
    "\n",
    "\n",
    "\n",
    "    entropy_maps = []\n",
    "    for i, mask_pred in enumerate( masks_pred[0]):\n",
    "  \n",
    "\n",
    "        p = mask_pred.clamp(1e-6, 1 - 1e-6)\n",
    "        if p.ndim == 2:\n",
    "            p = p.unsqueeze(0)\n",
    "    \n",
    "        entropy_map = - (p * torch.log(p) + (1 - p) * torch.log(1 - p))\n",
    "        entropy_map = entropy_map.max(dim=0)[0]\n",
    "        pred_mask = p.max(dim=0)[0]\n",
    "        pred_bin = (pred_mask > 0.5).to(torch.uint8).cpu().numpy() * 255\n",
    "\n",
    "        entropy_maps.append(entropy_map)\n",
    "    \n",
    "        entropy_np = entropy_map.cpu().numpy()\n",
    "        entropy_norm = (entropy_np - entropy_np.min()) / (entropy_np.max() - entropy_np.min() + 1e-6)\n",
    "        cmap = cm.get_cmap(\"viridis\")\n",
    "        entropy_color = (cmap(entropy_norm)[:, :, :3] * 255).astype(np.uint8)\n",
    "\n",
    "    \n",
    "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        suffix = f\"{rank:05d}_{base}\"\n",
    "        Image.fromarray(render['real']).save(os.path.join(save_dir, f\"{suffix}{i}.jpg\"))\n",
    "        Image.fromarray(pred_bin).save(os.path.join(save_dir, f\"{suffix}{i}_pred.jpg\"))\n",
    "        Image.fromarray(entropy_color).save(os.path.join(save_dir, f\"{suffix}{i}_en.jpg\"))\n",
    "    new_prom = neg_prompt_calibration(cfg, entropy_maps, prompt_b) \n",
    "\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     _, masks_pred, _, _ = model(img_tensor, new_prom)\n",
    "\n",
    "    # entropy_maps = []\n",
    "    # for i, mask_pred in enumerate( masks_pred[0]):\n",
    "  \n",
    "\n",
    "    #     p = mask_pred.clamp(1e-6, 1 - 1e-6)\n",
    "    #     if p.ndim == 2:\n",
    "    #         p = p.unsqueeze(0)\n",
    "    \n",
    "    #     entropy_map = - (p * torch.log(p) + (1 - p) * torch.log(1 - p))\n",
    "    #     entropy_map = entropy_map.max(dim=0)[0]\n",
    "    #     pred_mask = p.max(dim=0)[0]\n",
    "    #     pred_bin = (pred_mask > 0.5).to(torch.uint8).cpu().numpy() * 255\n",
    "\n",
    "    #     entropy_maps.append(entropy_map)\n",
    "    \n",
    "    #     entropy_np = entropy_map.cpu().numpy()\n",
    "    #     entropy_norm = (entropy_np - entropy_np.min()) / (entropy_np.max() - entropy_np.min() + 1e-6)\n",
    "    #     cmap = cm.get_cmap(\"viridis\")\n",
    "    #     entropy_color = (cmap(entropy_norm)[:, :, :3] * 255).astype(np.uint8)\n",
    "\n",
    "    \n",
    "    #     base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    #     suffix = f\"{rank:05d}_{base}\"\n",
    "    #     Image.fromarray(render['real']).save(os.path.join(save_dir, f\"{suffix}{i}_a.jpg\"))\n",
    "    #     Image.fromarray(pred_bin).save(os.path.join(save_dir, f\"{suffix}{i}_pred_a.jpg\"))\n",
    "    #     Image.fromarray(entropy_color).save(os.path.join(save_dir, f\"{suffix}{i}_en_a.jpg\"))\n",
    "\n",
    "    \n",
    "    if rank>0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb382e73-510f-440f-9522-96d091306df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c263f790-c4b3-4c1f-a2f1-6d7d222669ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, random\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import matplotlib.cm as cm\n",
    "\n",
    "# save_dir = \"entropy_sorted\"\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# def shift_and_resize_prompt(prompt, crop_x, crop_y, crop_size, orig_w, orig_h):\n",
    "#     \"\"\"\n",
    "#     Shift prompt points based on crop (x, y), \n",
    "#     keep points inside the crop, and resize them to (orig_w, orig_h).\n",
    "#     \"\"\"\n",
    "#     points = prompt[0][0]  # tensor shape: [N, 2]\n",
    "#     labels = prompt[0][1]  # tensor shape: [N] or [N, 1]\n",
    "\n",
    "#     # Shift coordinates by crop offset\n",
    "#     points[:, 0] -= crop_x\n",
    "#     points[:, 1] -= crop_y\n",
    "\n",
    "#     # Keep points inside the crop region\n",
    "#     mask_inside = (\n",
    "#         (points[:, 0] >= 0) & (points[:, 1] >= 0) &\n",
    "#         (points[:, 0] < crop_size) & (points[:, 1] < crop_size)\n",
    "#     )\n",
    "#     points = points[mask_inside]\n",
    "#     labels = labels[mask_inside]\n",
    "\n",
    "#     if len(points) == 0:\n",
    "#         # If no valid points remain, return dummy far-out point\n",
    "#         points = torch.tensor([[orig_w + 1000, orig_h + 1000]], device=prompt[0].device)\n",
    "#         labels = torch.tensor([0], device=prompt[0].device)\n",
    "\n",
    "#     # Rescale points from crop_size → original image size (since resized)\n",
    "#     scale_x = orig_w / crop_size\n",
    "#     scale_y = orig_h / crop_size\n",
    "#     points[:, 0] *= scale_x\n",
    "#     points[:, 1] *= scale_y\n",
    "\n",
    "#     return (points, labels)\n",
    "\n",
    "# patch_size = 256  # size of random patch before resize\n",
    "# num_patches = 5   # number of random patches per image\n",
    "\n",
    "# def get_random_patch_coords(h, w, patch_size):\n",
    "#     y = random.randint(0, max(0, h - patch_size))\n",
    "#     x = random.randint(0, max(0, w - patch_size))\n",
    "#     return y, x\n",
    "\n",
    "# for rank, (entropy_scalar, img_path, render) in enumerate(collected, start=1):\n",
    "#     img = render['real']\n",
    "#     H, W = img.shape[:2]\n",
    "\n",
    "#     img_tensor_full = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "#     img_tensor_full = img_tensor_full.unsqueeze(0).to(fabric.device)\n",
    "\n",
    "#     prompt_b = render['prompt']\n",
    "\n",
    "#     pseudo_accum = torch.zeros((1, H, W), dtype=torch.float32, device=fabric.device)\n",
    "#     count = 0\n",
    "\n",
    "#     # === Random patch extraction + resized inference ===\n",
    "#     for _ in range(num_patches):\n",
    "#         y, x = get_random_patch_coords(H, W, patch_size)\n",
    "#         patch = img[y:y+patch_size, x:x+patch_size, :]\n",
    "\n",
    "#         # --- Resize patch to full original image size before model inference ---\n",
    "#         patch_resized = np.array(Image.fromarray(patch).resize((W, H), Image.BILINEAR))\n",
    "#         patch_tensor = torch.from_numpy(patch_resized).permute(2, 0, 1).float() / 255.0\n",
    "#         patch_tensor = patch_tensor.unsqueeze(0).to(fabric.device)\n",
    "\n",
    "#         ref_prompt = shift_and_resize_prompt(prompt_b, x, y, patch_size, W, H)  # same or adaptive prompt\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             _, masks_pred, _, _ = model(patch_tensor, ref_prompt)\n",
    "#             p = masks_pred[0].clamp(1e-6, 1 - 1e-6)\n",
    "#             p = p.max(dim=0)[0]\n",
    "#             pseudo_accum += p.unsqueeze(0)\n",
    "#             count += 1\n",
    "\n",
    "#     # === Average pseudo-label across all resized patch predictions ===\n",
    "#     pseudo_mask = pseudo_accum / (count + 1e-6)\n",
    "\n",
    "#     # === Entropy computation ===\n",
    "#     entropy_map = - (pseudo_mask * torch.log(pseudo_mask + 1e-6) +\n",
    "#                      (1 - pseudo_mask) * torch.log(1 - pseudo_mask + 1e-6))\n",
    "#     entropy_np = entropy_map.squeeze(0).cpu().numpy()\n",
    "#     entropy_norm = (entropy_np - entropy_np.min()) / (entropy_np.max() - entropy_np.min() + 1e-6)\n",
    "#     cmap = cm.get_cmap(\"viridis\")\n",
    "#     entropy_color = (cmap(entropy_norm)[:, :, :3] * 255).astype(np.uint8)\n",
    "\n",
    "#     # === Binarized pseudo mask ===\n",
    "#     pseudo_bin = (pseudo_mask > 0.5).to(torch.uint8).squeeze(0).cpu().numpy() * 255\n",
    "\n",
    "#     base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "#     suffix = f\"{rank:05d}_{base}\"\n",
    "\n",
    "#     Image.fromarray(img).save(os.path.join(save_dir, f\"{suffix}.jpg\"))\n",
    "#     Image.fromarray(pseudo_bin).save(os.path.join(save_dir, f\"{suffix}_pseudo.jpg\"))\n",
    "#     Image.fromarray(entropy_color).save(os.path.join(save_dir, f\"{suffix}_entropy.jpg\"))\n",
    "\n",
    "#     # === Optional: negative prompt recalibration ===\n",
    "#     # new_prom = neg_prompt_calibration(cfg, [entropy_map], prompt_b)\n",
    "\n",
    "#     if rank > 0:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82482b0a-98f5-4a2b-9033-9b211534dfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c237be62-e469-4655-8822-a0be202fa5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01301e45-b4d2-4615-b3f0-0be198502a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef8e4fb-5aa2-45e9-af88-ca0651d157c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
